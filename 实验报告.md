# AM-GAML实验报告

## 1. 实验概述

### 1.1 实验目标
本实验旨在开发一种结合LSTM、SVM和图神经网络(GNN)的混合方法AM-GAML，用于金融异常检测任务。通过多种baseline方法的对比，验证AM-GAML方法的有效性。

### 1.2 方法架构
AM-GAML  是一种创新的多模态融合方法：
- **LSTM组件**：处理客户交易序列，提取时序特征
- **SVM组件**：基于LSTM特征进行图边构建
- **GNN组件**：在构建的图上进行节点分类

## 2. 数据集信息

### 2.1 数据概况
- **数据源**：金融交易数据
- **特征维度**：17个特征，包括客户类型、业务代码、基金代码、确认金额、时间特征、客户属性等
- **分类特征**：CUST_TYPE, BUSI_CODE, FUND_CODE, GENDER, NET_CODE, COUNTY_PROV, COUNTY_CITY, COUNTY_DIST, FLAG, CONF_YEAR, CONF_MONTH, CONF_DAY
- **数值特征**：AGE, RISK_LEV, CONF_AMTS, 5D_TOTAL, TELL_PREFIX

### 2.2 数据划分
- **训练集**：40%
- **验证集**：30% 
- **测试集**：30%
- 按客户类型分层划分，确保各数据集类别分布均衡

## 3. Baseline方法

### 3.1 已实现的Baseline
1. **LSTM** - 长短期记忆网络
2. **SVM** - 支持向量机
3. **XGBoost** - 极端梯度提升
4. **GBDT** - 梯度提升决策树
5. **GraphSAGE** - 图采样聚合网络
6. **GAT** - 图注意力网络

### 3.2 待实现的Baseline
1. **PC-GNN** - Principal Component Graph Neural Network
2. **Care-GNN** - Enhancing Graph Neural Networks with Attention

## 4. 实验设置

### 4.1 评估指标
所有方法使用统一的评估指标：
- **Accuracy** - 准确率
- **Precision** - 精确率 (macro平均)
- **Recall** - 召回率 (macro平均)  
- **F1 Score** - F1分数 (macro平均)
- **AUPRC** - 精确率-召回率曲线下面积

### 4.2 模型参数
- **LSTM**: hidden_size=64, num_epochs=101, batch_size=64, early_stopping=100
- **SVM**: kernel='rbf', C=3.0, gamma='scale', class_weight='balanced'
- **XGBoost**: n_estimators=200, max_depth=5, learning_rate=0.1
- **GBDT**: n_estimators=200, max_depth=5, learning_rate=0.1
- **GraphSAGE**: hidden_channels=64, epochs=100
- **GAT**: hidden_channels=64, heads=1, epochs=500

### 4.3 随机种子
所有实验固定随机种子为42，确保结果可重现。

## 5. AM-GAML方法详述

### 5.1 第一阶段：LSTM特征提取
1. **序列构建**：按客户ID分组，构建交易序列
2. **特征工程**：
   - 金额取对数变换：`log1p(CONF_AMTS)`, `log1p(5D_TOTAL)`
   - 分类变量标签编码
   - 数值变量MinMax归一化
3. **LSTM训练**：
   - 输入维度：14维特征向量
   - 隐藏层维度：64
   - 输出：客户级别的64维特征表示

### 5.2 第二阶段：SVM图构建
1. **特征标准化**：对LSTM输出的64维特征进行标准化
2. **SVM训练**：使用RBF核的SVM获得概率预测
3. **边构建**：
   - 计算SVM概率向量的余弦相似度
   - K=5近邻连接
   - 相似度阈值：0.7

### 5.3 第三阶段：GNN分类
1. **图构建**：
   - 节点特征：原始特征 + 编码分类特征 + LSTM特征 (73维)
   - 边：基于SVM相似度的无向边
2. **GNN架构**：
   - 使用GATv2Conv和SAGEConv交替堆叠
   - 4层图卷积 + 跳跃连接
   - 隐藏维度：128，注意力头数：4
   - Dropout：0.2

## 6. 实验结果

### 6.1 各方法性能对比

| 方法 | Accuracy | Precision | Recall | F1 Score | AUPRC |
|------|----------|-----------|--------|----------|-------|
| LSTM | - | - | - | - | - |
| SVM | - | - | - | - | - |
| XGBoost | - | - | - | - | - |
| GBDT | - | - | - | - | - |
| GraphSAGE | - | - | - | - | - |
| GAT | - | - | - | - | - |
| **AM-GAML** | - | - | - | - | - |

*注：具体数值需要运行完整实验后填入*

### 6.2 模型训练曲线
- 训练损失和验证指标变化趋势
- 早停机制触发情况
- 最佳模型选择策略

### 6.3 消融研究
1. **仅LSTM**：只使用LSTM特征进行分类
2. **LSTM+SVM**：使用SVM处理LSTM特征但不构建图
3. **完整AM-GAML**：LSTM + SVM + GNN的完整流程

## 7. 技术创新点

### 7.1 多模态特征融合
- 时序特征（LSTM）+ 传统机器学习特征（SVM）+ 图结构特征（GNN）
- 层次化特征学习策略

### 7.2 自适应图构建
- 基于SVM概率输出的语义相似度
- 动态阈值和K近邻策略
- 避免手工构建图结构的复杂性

### 7.3 端到端优化
- 虽然分阶段训练，但整体流程形成有效的特征传递链
- 每个组件都有明确的功能定位

## 8. 实验环境

### 8.1 硬件配置
- GPU: CUDA支持（如可用）
- CPU: 多核处理器
- 内存: 建议16GB以上

### 8.2 软件环境
- Python 3.8+
- PyTorch 1.12+
- PyTorch Geometric 2.0+
- scikit-learn 1.0+
- XGBoost 1.6+
- pandas, numpy, tqdm等

### 8.3 文件结构
```
AM-GAML/
├── data_new/                    # 数据文件
│   ├── preprocessed_data_gnn_full.csv
│   ├── customer_node_features_gnn.csv
│   ├── train_ids.csv, val_ids.csv, test_ids.csv
│   ├── homogeneous_graph_5.pt
│   └── hetero_graph.pt
├── for_LSTM.py                  # LSTM实现
├── for_SVM.py                   # SVM baseline
├── for_XGBoost.py              # XGBoost baseline  
├── for_SVM_ho.py               # 同构图构建
├── GNN_DS.py                   # 同构GNN实现
├── gnn_models.py               # 异构GNN实现
└── model/                      # 保存的模型
```

## 9. 结论与展望

### 9.1 主要贡献
1. 提出了AM-GAML多模态融合框架
2. 设计了基于SVM的自适应图构建方法
3. 在金融异常检测任务上验证了方法有效性

### 9.2 未来工作
1. 完成PC-GNN和Care-GNN的baseline实现
2. 进行更全面的超参数调优
3. 在更多数据集上验证方法泛化性
4. 探索端到端联合训练策略

## 10. 参考文献

1. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.
2. Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine learning, 20(3), 273-297.
3. Chen, T., & Guestrin, C. (2016). Xgboost: A scalable tree boosting system. KDD.
4. Hamilton, W., Ying, Z., & Leskovec, J. (2017). Inductive representation learning on large graphs. NIPS.
5. Veličković, P., et al. (2017). Graph attention networks. ICLR.

---

*报告生成日期：2025年7月20日*
*实验状态：部分完成，等待PC-GNN和Care-GNN实现*
