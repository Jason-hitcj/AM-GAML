import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import lightgbm as lgb
import xgboost as xgb
from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier


import pandas as pd
from sklearn.model_selection import train_test_split

# è½½å…¥æ•°æ®
# df = pd.read_csv('data/preprocessed_data_ml_full.csv')
# target_column = 'target'

# # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
# X = df.drop(target_column, axis=1)
# y = df[target_column]

df = pd.read_csv('data/customer_node_features_supervised.csv')
target_column = 'label'


# åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
# æå–ç‰¹å¾å¾—åˆ° numpy æ•°ç»„
features_numpy = df[[f'feature_{i}' for i in range(64)]].values

# å°† numpy æ•°ç»„è½¬æ¢å› DataFrame
columns = [f'feature_{i}' for i in range(64)]
X = pd.DataFrame(features_numpy, columns=columns)
y = df[target_column]

# æ£€æŸ¥ç‰¹å¾çŸ©é˜µ
print("\n=== ç‰¹å¾çŸ©é˜µæ£€æŸ¥ ===")
print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
print("\nç‰¹å¾æ•°æ®ç±»å‹:")
print(X.dtypes)
print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
print(X.isnull().sum())

# æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å¾ä¸ç›®æ ‡å˜é‡å®Œå…¨ä¸€è‡´
leakage_features = [col for col in X.columns if X[col].equals(y)]
if leakage_features:
    print(f"ğŸš¨ æ•°æ®æ³„éœ²ï¼ä»¥ä¸‹ç‰¹å¾ä¸ç›®æ ‡å˜é‡å®Œå…¨ç›¸åŒ: {leakage_features}")
else:
    print("âœ… æœªå‘ç°ç‰¹å¾ä¸ç›®æ ‡å˜é‡å®Œå…¨ä¸€è‡´")

# è®¡ç®—æ•°å€¼ç‰¹å¾ä¸ç›®æ ‡çš„ç»å¯¹ç›¸å…³æ€§
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
for col in numeric_features:
    corr = abs(X[col].corr(y))
    if corr > 0.99:
        print(f"âš ï¸ ç‰¹å¾ '{col}' ä¸ç›®æ ‡å˜é‡çš„ç»å¯¹ç›¸å…³æ€§é«˜è¾¾: {corr:.4f}")


# åˆ†å‰²æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.3, 
    random_state=9,
    stratify=y  # å¦‚æœåˆ†ç±»ä¸å¹³è¡¡å»ºè®®æ·»åŠ åˆ†å±‚æŠ½æ ·
)

print("\n=== æ•°æ®åˆ†å‰²ç»“æœ ===")
print(f"è®­ç»ƒé›†å½¢çŠ¶: X_train={X_train.shape}, y_train={y_train.shape}")
print(f"æµ‹è¯•é›†å½¢çŠ¶: X_test={X_test.shape}, y_test={y_test.shape}")

# ç¡®è®¤X_testå’Œy_testçš„IDä¸è®­ç»ƒé›†ä¸åŒ
print(f"X_train ID: {id(X_train)}, X_test ID: {id(X_test)}")
print(f"y_train ID: {id(y_train)}, y_test ID: {id(y_test)}")

# æ£€æŸ¥æµ‹è¯•é›†æ ·æœ¬æ˜¯å¦çœŸçš„ç‹¬ç«‹
assert len(set(X_test.index) & set(X_train.index)) == 0, "æµ‹è¯•é›†ä¸è®­ç»ƒé›†å­˜åœ¨é‡å æ ·æœ¬ï¼"

# å¦‚æœéœ€è¦åˆå¹¶ä¸ºå®Œæ•´DataFrameç”¨äºåˆ†æ/å¯è§†åŒ–
train_df = X_train.copy()
train_df[target_column] = y_train

test_df = X_test.copy()
test_df[target_column] = y_test

print("\nè®­ç»ƒé›†DataFrameé¢„è§ˆ:")
print(train_df.head())


# 1. Gradient Boosting Decision Tree (GBDT)

gbdt_model = GradientBoostingClassifier(n_estimators=100, random_state=42)
gbdt_model.fit(X_train, y_train)
y_pred_gbdt = gbdt_model.predict(X_test)



# 2. LightGBM

lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=42)
lgb_model.fit(X_train, y_train)
y_pred_lgb = lgb_model.predict(X_test)



# 3. XGBoost

xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)



# 4. AdaBoost

ada_model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3), n_estimators=100, random_state=42)
ada_model.fit(X_train, y_train)
y_pred_ada = ada_model.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
# å¦‚æœæ˜¯äºŒåˆ†ç±»ä»»åŠ¡ï¼Œå¯ä»¥ç®€åŒ–æŒ‡æ ‡è®¡ç®—ï¼š
def evaluate_model(y_true, y_pred, model_name):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    
    try:
        auc_roc = roc_auc_score(y_true, y_pred)
        print(f"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC-ROC: {auc_roc:.4f}")
    except:
        print(f"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")


print("Performance Comparison:")

evaluate_model(y_test, y_pred_gbdt, "GBDT")
evaluate_model(y_test, y_pred_lgb, "LightGBM")
evaluate_model(y_test, y_pred_xgb, "XGBoost")
evaluate_model(y_test, y_pred_ada, "AdaBoost")


plt.style.use('ggplot')  # ä½¿ç”¨æœ‰æ•ˆçš„æ ·å¼
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
axes = axes.flatten()



def plot_predictions(ax, y_true, y_pred, model_name):
    sns.scatterplot(x=y_true, y=y_pred, ax=ax)
    ax.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'k--', label='Perfect Prediction')
    ax.set_title(f'{model_name} Predictions vs Actual')
    ax.set_xlabel('Actual Values')
    ax.set_ylabel('Predicted Values')
    ax.legend()



# é¢„æµ‹ä¸å®é™…å€¼å¯¹æ¯”

plot_predictions(axes[0], y_test, y_pred_gbdt, 'GBDT')
plot_predictions(axes[1], y_test, y_pred_lgb, 'LightGBM')
plot_predictions(axes[2], y_test, y_pred_xgb, 'XGBoost')
plot_predictions(axes[3], y_test, y_pred_ada, 'AdaBoost')



plt.tight_layout()
# ä¿å­˜æ–‡ä»¶ï¼ˆæ”¯æŒPNG/PDF/SVGç­‰æ ¼å¼ï¼‰
plt.savefig("img/output1.png", dpi=300, bbox_inches="tight")  # bbox_inchesé˜²æ­¢æˆªæ–­

# ä¸»åŠ¨å…³é—­å›¾å½¢ï¼Œé¿å…å†…å­˜å ç”¨
plt.close()  # å…³é”®ï¼é˜»æ­¢å›¾å½¢æ˜¾ç¤º



# é¢„æµ‹å€¼åˆ†å¸ƒå›¾

fig, axes = plt.subplots(2, 2, figsize=(16, 12))
axes = axes.flatten()



def plot_prediction_distribution(ax, y_pred, model_name):
    sns.histplot(y_pred, ax=ax, kde=True, bins=30)
    ax.set_title(f'{model_name} Predicted Values Distribution')
    ax.set_xlabel('Predicted Values')
    ax.set_ylabel('Frequency')



# ç»˜åˆ¶é¢„æµ‹å€¼çš„åˆ†å¸ƒ

plot_prediction_distribution(axes[0], y_pred_gbdt, 'GBDT')
plot_prediction_distribution(axes[1], y_pred_lgb, 'LightGBM')
plot_prediction_distribution(axes[2], y_pred_xgb, 'XGBoost')
plot_prediction_distribution(axes[3], y_pred_ada, 'AdaBoost')


plt.tight_layout()
# ä¿å­˜æ–‡ä»¶ï¼ˆæ”¯æŒPNG/PDF/SVGç­‰æ ¼å¼ï¼‰
plt.savefig("img/output2.png", dpi=300, bbox_inches="tight")  # bbox_inchesé˜²æ­¢æˆªæ–­

# ä¸»åŠ¨å…³é—­å›¾å½¢ï¼Œé¿å…å†…å­˜å ç”¨
plt.close()  # å…³é”®ï¼é˜»æ­¢å›¾å½¢æ˜¾ç¤º

def plot_residuals(y_true, y_pred, model_name):
    plt.figure(figsize=(8, 6))
    residuals = y_true - y_pred
    sns.scatterplot(x=y_pred, y=residuals)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.title(f'{model_name} Residual Plot')
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.show()

# åˆ†åˆ«ç»˜åˆ¶æ¯ä¸ªæ¨¡å‹çš„æ®‹å·®å›¾
plot_residuals(y_test, y_pred_gbdt, 'GBDT')
plot_residuals(y_test, y_pred_lgb, 'LightGBM')
plot_residuals(y_test, y_pred_xgb, 'XGBoost')
plot_residuals(y_test, y_pred_ada, 'AdaBoost')
plt.tight_layout()
# ä¿å­˜æ–‡ä»¶ï¼ˆæ”¯æŒPNG/PDF/SVGç­‰æ ¼å¼ï¼‰
plt.savefig("img/output3.png", dpi=300, bbox_inches="tight")  # bbox_inchesé˜²æ­¢æˆªæ–­

# ä¸»åŠ¨å…³é—­å›¾å½¢ï¼Œé¿å…å†…å­˜å ç”¨
plt.close()  # å…³é”®ï¼é˜»æ­¢å›¾å½¢æ˜¾ç¤º
